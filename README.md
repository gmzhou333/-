# 机器学习
# 机器学习理论（参考西瓜书）
1.什么是机器学习？
 ----
  它致力于研究如何通过计算的手段，利用经验来改善自身的性能。在计算机系统中，“经验”通常以“数据”
  形式存在，因此，机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”的算法，即“学习
  算法”。有了学习算法，把经验数据提供给它，就能基于这些数据产生模型；面对新的情况时，模型就会
  给我们提供相应的判断。本书用“模型”泛指从数据中学得的结果。
  
2.学习与训练
---
  从数据中学得的模型的过程成为“学习”(learning)或“训练”(training)，这个过程通过执行某个学习算法
  来完成。训练过程中使用的数据称为训练数据，训练样本组成的集合称为“训练集”。学得模型对应了关于
  数据的某种潜在的规律，称为“假设”；这种潜在规律自身，则成为“真相”(ground-truth),学习过程就是
  找出或逼近真相。本书中有时将模型称为“学习器”(learner)。学得模型后，使用其进行预测的过程称为
  “测试”(testing)，被预测的样本被称为“测试样本”(testing sample)。
  
 
3.有监督学习(supervised learning)和无监督学习(unsupervised learning)
---
  分类和回归是前者的代表，聚类是后者的代表。
  期望学得的模型有很强的泛化能力，也就是它能很好地适用于整个样本空间。尽管训练集只是样本空间的
  一个很小的采样，训练者仍希望它能够很好地反映样本空间的特性。
  
4.机器学习的发展
---
  机器学习是人工智能研究发展到一定阶段的必然产物。二十世纪五十年代到七十年代初，赋予机器逻辑推
  理能力，机器就具有智能，称为“推理期”。二十世纪七十年代中期开始，人工智能进入了“知识期”，大量
  专家系统问世。在二十世纪八十年代，“从样例中学习”，被研究最多、应用最广，也就是广义的归纳学习，
  即从训练样例中归纳出学习结果，它涵盖了监督学习、无监督学习等。
  
  二十世纪八十年代，“从样例中学习”的一大主流是符号主义学习，其代表包括决策树和基于逻辑的学习。
  直到九十年代中期之前，“从样例中学习”的另一主流技术是基于神经网络的连接主义学习。二十世纪九十
  年代中期，“统计学习”迅速占据主流舞台，代表性技术是支持向量机以及更一般的核方法。二十一世纪初，
  连接主义卷土重来，掀起了以“深度学习”为名的热潮。深度学习，狭义地说就是“很多层”的神经网络。
  
5.过拟合和欠拟合
---
  学习器在训练集上的误差称为“训练误差”或“经验误差”，在新样本上的误差称为“泛化误差”。显然，希望得
  到泛化误差小的学习器。然而，我们事前并不知道新样本是什么样，能做到的是努力使经验误差最小化。
  我们希望，学习器能在新样本上表现得很好。当学习器对训练样本学习的太好了的时候，很可能已经把训练
  样本自身一些特点当做了潜在样本具有的一般性质，导致泛化能力下降，称为“过拟合”。
  
6.模型评估方法
---
  通常，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。需要使用“测试集”来测试学习
  器对新样本的判别能力，然后以测试集上的“测试误差”作为泛化误差的近似。通常我们假设测试样本也是从
  样本真实分布中独立同分布采样而得。
  
  -实验评估方法
  
  - 有时我们只有一个包含m个样例的数据集，既要训练又要测试，怎么样才能做到呢？
  
  - 留出法(hold-out) 2/3~4/5用作训练集，余下的用于测试集保持训练集和测试集数据分布的一致性，例如
    在分类任务中至少要保持样本类别比例相似。
    
  - 交叉验证法(cross validation) 每次用k-1个子集的并集作为训练集，余下的子集作为测试集，共k次。
  
  - 留一法(leave one out) m个样本唯一划分成m个子集，一个样本作为测试集，余下的用作训练集。缺点是
    计算开销难以忍受。
    
  - 自助法(bootstrapping) 我们希望评估用D训练出的模型。但在留出法和交叉验证法保留了一部分样例用于测
    试，因此实际评估模型所使用的训练集比D小，会引入因训练样本规模不同而导致的偏差，此时可以引入自助法。
    也成可放回重复采样。每次随机从D样本中挑出一个样本，可放回重复抽取m次，得到新样本D^，有的样本会重复
    出现，有的则不会出现。这样，实际评估的模型与期望评估的模型都是用m个训练样本，而仍约有1/3的样本没有
    出现在训练集中，可用于测试。
    
  - 自助法在数据集较小、难以划分训练测试集是很有用；此外，自助法能从初始数据集中产生很多个不同的训练集，
    对集成学习等方法有很大的好处。然而，自助法产生的数据集改变了初始数据集的分布，会引入估计偏差。因此，
    在初始数据量足够时，留出法和交叉验证法更常用一些。
    
  另外，我们通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，模型评估与选择中用于评估测试
  的数据集常称为“验证集”(validation set)。例如，在研究对比不同算法的泛化性能时，我们用测试集上的判别效果
  来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择
  和调参。
  
7.性能度量
---
  - 性能度量
  
  > 错误率与精度 分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。错误率是样本中分类
    错误的样本数占样本总数的比率。精度则是1-错误率。
    
  > 查准率、查全率与F1
  
    错误率精度虽然常用，但并不能满足所有任务需求。在二分类中，分类结果可以用“混淆矩阵”表示。
    我们经常会关心“检索的信息中有多少比例是用户感兴趣的”  “用户感兴趣的信息中有多少被检测出来”。
    
    查准率P = TP/(TP+FP)   
    查准率R = TP/(TP+FN)   
    查准率与查全率是一对矛盾的度量。既然是相互矛盾的值，在评估模型时，用到的是F1度量，综合了查准率和查全率。
   
    F = (1+beta^2)*P*R/(beta^2*P)+R
    beta>1 查全率有更大影响；beta<1时查准率有更大影响；beta=1时退化为标准的F1。
    
  > ROC和AUC
  
    很多学习器是为测试样本产生一个实值或预测概率，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正
    类，否则为反类。进行学习器比较时，若一个学习器的ROC曲线被另一个学习器的曲线完全包住，则可断言后者的性能
    优于前者；若两个学习器的ROC曲线发生交叉，很难判断孰优孰劣，此时则利用AUC进行判别。
    
  > 代价敏感错误率与代价曲线
  
    在现实任务中，不同类型的错误所造成的后果不同，前面的一些性能度量，大都隐式地假设了均等代价，并没有考虑不
    同错误会造成不同后果。在非均等代价中，我们所希望的不再是简单地最小化错误次数，而是希望最小化“总体代价”。
    
8.比较检验
---
  若在测试集上观察到学习器A比学习器B好，则A的泛化性能是否在统计意义上优于B，以及这个结论的把握有多大。
  > 偏差与方差
  
    以回归任务为例，期望输出与真实标记的差别称为偏差。
    f(x;D)为训练集D上学得模型f在x上的预测输出，y_D为x的数据集中的标记，y为x的真实标记；有可能出现噪声使得
    y_D != y
    
    学习算法的期望预测为：
    fbar(x) = E(f(x;D))
    
    使用样本数相同的不同训练集产生的方差为：
    var(x) = E_D(f(x;D)-fbar(x))^2
    
    噪声为
    e^2 = E_D(y_D-y)^2
    
    期望输出与真是标记的差别称为偏差：
    bias^2(x) = (fbar(x)-y)^2
    
    偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；方差度量了同样大小的
    训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；
  
  













